---
title: "Generation Server：语言模型生成"
---

## Server作用

Generation Server 用于部署大语言模型（LLM），并通过接收 Prompt Server 提供的输入完成响应生成。当前使用 [vLLM](https://github.com/vllm-project/vllm) 作为模型部署后端。

## 参数说明

```yaml
# servers/generation/parameter.yaml
model_name: openbmb/MiniCPM4-8B # model name or path
base_url: http://localhost:8000/v1 # vllm server url

# init vllm server configs
port: 8000
gpu_ids: "0,1"
api_key: ""

# generation parameters
sampling_params:
  temperature: 0.7
  top_p: 0.8
  max_tokens: 2048
  extra_body:
    top_k: 20
    chat_template_kwargs:
      enable_thinking: false # as qwen3, switch to true if you want to enable thinking
    include_stop_str_in_output: true # use in search-o1 pipeline
  # stop: [ "<|im_end|>", "<|end_search_query|>" ] # use in search-o1 pipeline
```

- `model_name`：使用的生成模型名称或路径
- `base_url`：vLLM 模型服务的 HTTP 接口地址
- `port`：本地 vLLM 服务监听的端口
- `gpu_ids`：指定gpu设备
- `api_key`：调用模型服务所需的 API Key
- `sampling_params`：vLLM 支持的生成参数，如温度、top-p、最大生成长度等；

## 工具说明

- `initialize_local_vllm`：在本地启动一个 vLLM 模型服务，并等待其准备就绪，最终返回该服务的 base_url。
- `generate`：接收 prompt server 提供的 prompt 输入，调用支持 OpenAI API 协议的 LLM 接口进行生成，最终返回回答字符串列表。