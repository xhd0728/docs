---
title: "Evaluation Server"
icon: "clipboard-check"
---

## Server Function

The Evaluation Server provides a complete set of text evaluation tools, supporting a variety of common metrics, and is used for automated evaluation of model outputs within the pipeline.

## Parameter Description

```yaml
# servers/evaluation/parameter.yaml
metrics: [ 'acc', 'f1', 'em', 'coverem', 'stringem', 'rouge-1', 'rouge-2', 'rouge-l' ]
save_path: output/asqa.json
```

- `metrics`: Specifies the evaluation metrics to be calculated; multiple metrics can be calculated simultaneously.
- `save_path`: Output path for storing the evaluation result log.

## Tool Description

- `evaluate`: Evaluates a set of model-generated answers and saves the evaluation results.

| Metric Name | Type   | Description                                                       |
|-------------|--------|-------------------------------------------------------------------|
| `EM`        | float  | Exact Match. Prediction exactly matches any reference.            |
| `Acc`       | float  | Prediction contains any form of the reference answer (lenient match). |
| `StringEM`  | float  | Soft match ratio for multiple answers (often used for multi-choice/nested QA). |
| `CoverEM`   | float  | Whether the reference answer is fully covered by the predicted text. |
| `F1`        | float  | Token-level F1 score.                                             |
| `Rouge_1`   | float  | 1-gram ROUGE-F1.                                                  |
| `Rouge_2`   | float  | 2-gram ROUGE-F1.                                                  |
| `Rouge_L`   | float  | Longest Common Subsequence (LCS) based ROUGE.                     |