---
title: "Prompt Server"
icon: "terminal"
---

## What is a Prompt Tool?

A Prompt Tool is a component in UltraRAG-MCP used for constructing inputs for language models.

Each Prompt Tool is defined by the `@app.prompt` decorator. Its main responsibility is to load a template file and generate standardized prompt messages based on inputs such as questions and retrieved passages. These prompt messages can be directly passed to an LLM for generation.

## How to Implement a Prompt Tool?

Implementing a Prompt Tool typically involves the following three steps:

### Step 1: Prepare the Prompt Template (Jinja2 format)

Save your prompt template as a file ending with `.jinja`, for example:

```jinja
Please answer the following question based on the given documents.
Think step by step.
Provide your final answer in the format \boxed{answer}.

Documents:
{{documents}}

Question: {{question}}
```

### Step 2: Implement the Tool in the Prompt Server

Call our provided `load_prompt_template` method to load the template, and implement a tool function (Tool) in the Prompt Server to assemble the prompt.

```python
@app.prompt(output="q_ls,ret_psg,template->prompt_ls")
def qa_rag_boxed(
    q_ls: List[str], ret_psg: List[str | Any], template: str | Path
) -> list[PromptMessage]:
    template: Template = load_prompt_template(template)
    ret = []
    for q, psg in zip(q_ls, ret_psg):
        passage_text = "\n".join(psg)
        p = template.render(question=q, documents=passage_text)
        ret.append(p)
    return ret
```

### Step 3: Use the Tool in the Pipeline

1. Register the Prompt Server in your YAML configuration file and call the Tool you implemented:

```yaml
# MCP Server
servers:
  generation: servers/generation
  prompt: servers/prompt

# MCP Client Pipeline
pipeline:
- prompt.qa_rag_boxed
- generation.generate
```

2. After building, modify the runtime parameters to specify your template file path:

```yaml
prompt:
  template: prompt/qa_rag_boxed.jinja
```

ðŸš© In this way, you can achieve custom prompt construction and invocation, greatly enhancing the reusability and extensibility of LLM application workflows.