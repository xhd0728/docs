---
title: "RAG Workflow Based on Multi-turn Intermediate Results"
icon: "layer-group"
---

This section will implement a more complex multi-turn RAG reasoning workflow — IRCoT (Iterative Retrieval with Chain-of-Thought).

> For the original paper, see: https://arxiv.org/pdf/2212.10509

The core idea of IRCoT is: in each round, the model generates new reasoning content (CoT) based on the currently retrieved documents, the history of reasoning chains, and the question, and then uses this to trigger the next round of retrieval. This alternating loop can continuously deepen the reasoning process until a termination condition is met (such as an explicit answer being generated). Therefore, it requires effective recording and access to multi-turn intermediate results — which is exactly where UltraRAG's Memory mechanism comes into play.

## Step 1: Clarify the Workflow Structure

The IRCoT reasoning process includes the following steps:

1. Initial retrieval: use the original question as a query to get the first batch of documents;
2. Alternating reasoning-retrieval loop (up to N rounds):
  - Use the current retrieved documents + historical CoT reasoning sentences to generate the next CoT;
  - If the newly generated text contains "So the answer is:", terminate early;
  - Otherwise, extract the first sentence of the current CoT as the next retrieval query;
3. Final answer generation: after ending the iteration, extract the answer for evaluation.

To realize the above workflow, we need to extend UltraRAG's built-in features and add the following Tools:

- Prompt construction: supports concatenation of historical CoT + documents; the first round needs special handling as there is no CoT yet;
- Check for early termination: identify whether the final answer sentence has been generated (the original paper checks for the presence of "So the answer is:");
- Construct the next query: constructs the next retrieval query in each round, using the first sentence of the model-generated CoT;
- Answer extraction: extracts the final answer content from the generated text.

To access the previous CoT and retrieval history, you need to use UltraRAG's Memory intermediate variable storage mechanism. Simply prefix the variable name with `memory_` to access the intermediate results from previous iterations. For more on this, refer to the tutorial [Memory: UltraRAG's Intermediate Variable Storage Mechanism].

## Step 2: Implement Necessary Tools

### Step 2.1: Implement ircot_next_prompt

First, define the prompt template in `prompt/IRCoT.jinja`, for example:

```jinja
You serve as an intelligent assistant, adept at facilitating users through complex, multi-hop reasoning across multiple documents. This task is illustrated through demonstrations, each consisting of a document set paired with a relevant question and its multi-hop reasoning thoughts. Your task is to generate one thought for current step, DON'T generate the whole thoughts at once! If you reach what you believe to be the final step, start with "So the answer is:".

Wikipedia Title: Kurram Garhi
Kurram Garhi is a small village located near the city of Bannu, which is the part of Khyber Pakhtunkhwa province of Pakistan. Its population is approximately 35000. Barren hills are near this village. This village is on the border of Kurram Agency. Other nearby villages are Peppal, Surwangi and Amandi Kala.

Wikipedia Title: 2001–02 UEFA Champions League second group stage
Eight winners and eight runners- up from the first group stage were drawn into four groups of four teams, each containing two group winners and two runners- up. Teams from the same country or from the same first round group could not be drawn together. The top two teams in each group advanced to the quarter- finals.

Wikipedia Title: Satellite tournament
A satellite tournament is either a minor tournament or event on a competitive sporting tour or one of a group of such tournaments that form a series played in the same country or region.

Wikipedia Title: Trojkrsti
Trojkrsti is a village in Municipality of Prilep, Republic of Macedonia.

Wikipedia Title: Telephone numbers in Ascension Island
Country Code:+ 247< br> International Call Prefix: 00 Ascension Island does not share the same country code( +290) with the rest of St Helena.

Question: Are both Kurram Garhi and Trojkrsti located in the same country?
Thought: Kurram Garhi is located in the country of Pakistan. Trojkrsti is located in the country of Republic of Macedonia. Thus, they are not in the same country. So the answer is: no.

{{documents}}

Question: {{question}}
Thought: {{cur_answer}}
```

Then, add the following to the prompt server:

```python
# prompt for IRCOT
@app.prompt(output="memory_q_ls,memory_ret_psg,template->prompt_ls")
def ircot_next_prompt(
    memory_q_ls: List[List[str | None]],
    memory_ret_psg: List[List[List[str]] | None],
    template: str | Path,
) -> List[PromptMessage]:
    template: Template = load_prompt_template(template)
    ret: List[PromptMessage] = []
    # ---------- Single round ----------
    if len(memory_q_ls) == 1:
        for q, psg in zip(memory_q_ls[0], memory_ret_psg[0]):
            if q is None:
                continue
            passage_text = "" if psg is None else "\n".join(psg)
            ret.append(
                template.render(documents=passage_text, question=q, cur_answer="")
            )
        return ret
    # ---------- Multi-turn ----------
    data_num = len(memory_q_ls[0])
    round_cnt = len(memory_q_ls)
    for i in range(data_num):
        if memory_q_ls[0][i] is None:  # terminated sample
            continue
        all_passages, all_cots = [], []
        for r in range(round_cnt):
            psg = None
            if memory_ret_psg is not None and r < len(memory_ret_psg):
                round_psg = memory_ret_psg[r]
                if round_psg is not None and i < len(round_psg):
                    psg = round_psg[i]
            if psg:  
                all_passages.extend(psg)
            if r > 0:
                cot = memory_q_ls[r][i]
                if cot:
                    all_cots.append(cot)
        passage_text = "\n".join(all_passages)
        cur_answer = " ".join(all_cots).strip()
        q = memory_q_ls[0][i]
        ret.append(
            template.render(documents=passage_text, question=q, cur_answer=cur_answer)
        )
    return ret
```

This function will automatically concatenate the history of retrieval results and CoT for each round and pass them into the current template for rendering, constructing the model input.

### Step 2.2: Implement router.ircot_check_end

Used to check if the generated model content contains "So the answer is:":

```python
@app.tool(output="ans_ls->ans_ls")
def ircot_check_end(ans_ls: List[str]) -> Dict[str, List[Dict[str, str]]]:
    ans_ls = [
        {
            "data": ans,
            "state": "complete" if "so the answer is" in ans.lower() else "incomplete",
        }
        for ans in ans_ls
    ]
    return {"ans_ls": ans_ls}
```

Returns whether each sample is completed (state: complete / incomplete).

### Step 2.3: Implement custom.ircot_get_first_sent

Because IRCoT uses the first sentence of the current model-generated content for retrieval in each round, you need to implement a separate logic in custom for extracting the first sentence:

```python
@app.tool(output="ans_ls->q_ls")
def ircot_get_first_sent(
    ans_ls: List[str],
) -> Dict[str, List[str]]:
    ret = []
    for ans in ans_ls:
        match = re.search(r"(.+?[。！？.!?])", ans)
        if match:
            ret.append(match.group(1))
        else:
            ret.append(ans.strip())
    return {"q_ls": ret}
```

### Step 2.4: Implement custom.ircot_extract_ans

Used to extract the specific answer from the model's output in the last round:

```python
@app.tool(output="ans_ls->pred_ls")
def ircot_extract_ans(ans_ls: List[str]) -> Dict[str, List[str]]:
    ret = []
    pattern = re.compile(r"so the answer is[\s:]*([^\n]*)", re.IGNORECASE)
    for ans in ans_ls:
        match = pattern.search(ans)
        if match:
            ret.append(match.group(1).strip())
        else:
            ret.append(ans.strip())
    return {"pred_ls": ret}
```

## Step 3: Write the Pipeline Configuration File

Once the above tools are implemented, you can use the following YAML file to construct the IRCoT reasoning workflow:

```yaml
# MCP Server
servers:
  benchmark: servers/benchmark
  generation: servers/generation
  retriever: servers/retriever
  prompt: servers/prompt
  evaluation: servers/evaluation
  router: servers/router
  custom: servers/custom

# MCP Client Pipeline
pipeline:
- benchmark.get_data
# For the first n-1 rounds of retrieval, need to extract the first sentence of CoT
- loop:
    times: 2
    steps:
    # Retrieval Q->D
    - retriever.retriever_deploy_search
    # T_i = Reason(Q + D + T_i-1)
    - prompt.ircot_next_prompt
    - generation.generate
    - branch:
        router:
        # Check if "so the answer is" is present
        - router.ircot_check_end
        branches:
          incomplete:
          # Extract the first sentence as CoT
          - custom.ircot_get_first_sent
          complete: []
# For the nth round of retrieval, do not extract the first sentence
# T_3 = Reason(Q + D + T_2)
- retriever.retriever_deploy_search
- prompt.ircot_next_prompt
- generation.generate
- custom.ircot_extract_ans
- evaluation.evaluate
```

## Step 4: Configure Pipeline Parameters

Run the command to build the parameter template:

```shell
ultrarag build examples/IRCoT.yaml
```

Then edit the generated `examples/parameter/IRCoT_parameter.yaml` and configure as follows:

```yaml
benchmark:
  benchmark:
    key_map:
      gt_ls: golden_answers
      q_ls: question
    limit: 2
    name: asqa
    path: data/sample_asqa_5.jsonl
custom: {}
evaluation:
  metrics:
  - acc
  - f1
  - em
  - coverem
  - stringem
  - rouge-1
  - rouge-2
  - rouge-l
  save_path: output/asqa.json
generation:
  base_url: http://localhost:8000/v1
  model_name: openbmb/MiniCPM4-8B
  sampling_params:
    extra_body:
      chat_template_kwargs:
        enable_thinking: false
      include_stop_str_in_output: true
      top_k: 20
    max_tokens: 2048
    temperature: 0.7
    top_p: 0.8
prompt:
  template: prompt/qa_boxed.jinja
retriever:
  query_instruction: 'Query: '
  retriever_url: http://localhost:8080
  top_k: 5
```

## Step 5: Run Your Reasoning Workflow!

Once everything is ready, run the following command to start the reasoning workflow:

```shell
ultrarag run examples/IRCoT.yaml
```