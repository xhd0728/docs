---
title: "Serial RAG Workflow"
icon: "arrow-right-arrow-left"
---

This section will guide you step by step to implement the most basic RAG workflow: Vanilla RAG. You will learn how to use UltraRAG to build a reasoning system covering the complete process from data loading, retrieval, generation to evaluation.

## Step 1: Define the Workflow Structure

The basic process of Vanilla RAG is as follows:

> Data loading → Document retrieval → Model generation → Answer evaluation

To ensure the retrieval step works properly, please first complete corpus embedding and index building. See the tutorial: [Encoding and Indexing Large-scale Corpora with UltraRAG] for details.

## Step 2: Implement Necessary Tools

In this process, we want the model's final answer to be wrapped in \boxed{}, so that it can be automatically extracted later. Therefore, we need:

- A custom Prompt Tool: to construct standardized generation inputs from the question and retrieved contents;
- A custom Tool: to extract the answer text wrapped in \boxed{};
- All other modules (retrieval, generation, evaluation) can directly reuse UltraRAG’s existing components.

### Step 2.1: Implement prompt.qa_rag_boxed

First, prepare the prompt template file prompt/qa_boxed.jinja:

```jinja
Please answer the following question.
Think step by step.
Provide your final answer in the format \boxed{YOUR_ANSWER}.

Question: {{question}}
```

Then, implement the following Tool in the Prompt Server:

```python
# prompt for QA RAG boxed
@app.prompt(output="q_ls,ret_psg,template->prompt_ls")
def qa_rag_boxed(
    q_ls: List[str], ret_psg: List[str | Any], template: str | Path
) -> list[PromptMessage]:
    template: Template = load_prompt_template(template)
    ret = []
    for q, psg in zip(q_ls, ret_psg):
        passage_text = "\n".join(psg)
        p = template.render(question=q, documents=passage_text)
        ret.append(p)
    return ret
```

### Step 2.2: Implement custom.output_extract_from_boxed

To extract the answer wrapped in \boxed{} from the model output, implement the following Tool in the Custom Server:

```python
@app.tool(output="ans_ls->pred_ls")
def output_extract_from_boxed(ans_ls: List[str]) -> Dict[str, List[str]]:
    def extract(ans: str) -> str:
        start = ans.rfind(r"\boxed{")
        if start == -1:
            content = ans.strip()
        else:
            i = start + len(r"\boxed{")
            brace_level = 1
            end = i
            while end < len(ans) and brace_level > 0:
                if ans[end] == "{":
                    brace_level += 1
                elif ans[end] == "}":
                    brace_level -= 1
                end += 1
            content = ans[i : end - 1].strip()
            content = re.sub(r"^\$+|\$+$", "", content).strip()
            content = re.sub(r"^\\\(|\\\)$", "", content).strip()
            if content.startswith(r"\text{") and content.endswith("}"):
                content = content[len(r"\text{") : -1].strip()
            content = content.strip("()").strip()
        # Restore \\
        content = content.replace("\\", " ")
        content = content.replace("  ", " ")
        return content

    return {"pred_ls": [extract(ans) for ans in ans_ls]}
```

## Step 3: Write the Pipeline Configuration File

After completing the above code development, create a new configuration file in the examples/ directory: vanilla_rag.yaml.

```yaml
# Vanilla RAG

# MCP Server
servers:
  benchmark: servers/benchmark
  retriever: servers/retriever
  prompt: servers/prompt
  generation: servers/generation
  evaluation: servers/evaluation
  custom: servers/custom

# MCP Client Pipeline
pipeline:
- benchmark.get_data
# If retriever is not deployed, replace with the following two steps:
# - retriever.retriever_init      
# - retriever.retriever_search
- retriever.retriever_deploy_search
- prompt.qa_rag_boxed
- generation.generate
- custom.output_extract_from_boxed
- evaluation.evaluate
```

## Step 4: Configure Pipeline Parameters

Run the following command:

```shell
ultrarag build examples/vanilla_rag.yaml
```

Open the generated examples/parameter/vanilla_rag_parameter.yaml and modify the configuration as follows:

```yaml
benchmark:
  benchmark:
    key_map:
      gt_ls: golden_answers
      q_ls: question
    limit: 2
    name: asqa
    path: data/sample_asqa_5.jsonl
custom: {}
evaluation:
  metrics:
  - acc
  - f1
  - em
  - coverem
  - stringem
  - rouge-1
  - rouge-2
  - rouge-l
  save_path: output/asqa.json
generation:
  base_url: http://localhost:8000/v1
  model_name: openbmb/MiniCPM4-8B
  sampling_params:
    extra_body:
      chat_template_kwargs:
        enable_thinking: false
      include_stop_str_in_output: true
      top_k: 20
    max_tokens: 2048
    temperature: 0.7
    top_p: 0.8
prompt:
  template: prompt/qa_boxed.jinja
retriever:
  query_instruction: 'Query: '
  retriever_url: http://localhost:8080
  top_k: 5
```

## Step 5: Run Your Reasoning Workflow!

Once everything is ready, run the following command to start the reasoning workflow:

```shell
ultrarag run examples/vanilla_rag.yaml
```

### Evaluation Results

The model prediction results will be automatically evaluated and saved to the path configured in the evaluation server’s save_path. For example:

```shell
output/asqa.json
```

### Running Logs

Detailed logs of the reasoning process are saved in the logs/ directory. File names are generated based on runtime for easy tracking and reproducibility. For example:

```shell
logs/20250804_193900.log
```

### Intermediate Result Files

All intermediate results during workflow execution (including the input and output of each step) are recorded as memory files, by default saved in the output/ directory. For example:

```shell
output/memory_asqa_vanilla_rag_20250804_193900.json
```