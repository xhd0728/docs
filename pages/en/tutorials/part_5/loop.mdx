---
title: "Cyclic RAG Workflow"
icon: "rotate-right"
---

This section will guide you to implement a typical iterative reasoning RAG workflow: IterRetGen. This workflow supports repeatedly updating queries based on model outputs to gradually converge to a better answer.

> For the original paper, see: https://arxiv.org/pdf/2305.15294

## Step 1: Clarify the Workflow Structure

Let’s review the original IterRetGen workflow structure diagram from the paper:

![](/images/part_5/iterretgen.png)

In UltraRAG, you can quickly implement this workflow based on existing modules, and its overall architecture can be abstracted as follows:

![](/images/part_5/iterretgen_pipe.png)

As shown:

- The model’s answer generated in each round is concatenated with the original question to form the next round’s retrieval query.
- This process is executed N times according to the set maximum loop count.
- Except for the prompt and custom modules, which need custom implementation, all other functions can be directly reused from UltraRAG’s built-in tools.

## Step 2: Implement the Required Tool

### Step 2.1: Implement custom.iterretgen_nextquery

IterRetGen constructs each round’s query by concatenating the query and the current round’s answer. Add the following code to `servers/custom/src/custom.py`:

```python
@app.tool(output="q_ls,ret_psg->nextq_ls")
def iterretgen_nextquery(
    q_ls: List[str],
    ans_ls: List[str | Any],
) -> Dict[str, List[str]]:
    ret = []
    for q, ans in zip(q_ls, ans_ls):
        next_query = f"{q} {ans}"
        ret.append(next_query)
    return {"nextq_ls": ret}
```

## Step 3: Write the Pipeline Configuration File

Create a new YAML file in the `examples/` directory, such as `IterRetGen.yaml`:

```yaml
# IterRetGen demo

# MCP Server
servers:
  benchmark: servers/benchmark
  retriever: servers/retriever
  prompt: servers/prompt
  generation: servers/generation
  evaluation: servers/evaluation
  custom: servers/custom

# MCP Client Pipeline
pipeline:
- benchmark.get_data
- retriever.retriever_deploy_search
- prompt.qa_rag_boxed
- generation.generate
- custom.output_extract_from_boxed
- loop:
    times: 3
    steps:
    - custom.iterretgen_nextquery:
        input:
          ans_ls: pred_ls
    - retriever.retriever_deploy_search:
        input:
          query_list: nextq_ls
    - prompt.qa_rag_boxed
    - generation.generate
    - custom.output_extract_from_boxed
- evaluation.evaluate
```

## Step 4: Configure Pipeline Parameters

Run the following command:

```shell
ultrarag build examples/IterRetGen.yaml
```

Open the generated `examples/parameter/IterRetGen_parameter.yaml` and modify the configuration as follows:

```yaml
benchmark:
  benchmark:
    key_map:
      gt_ls: golden_answers
      q_ls: question
    limit: 2
    name: asqa
    path: data/sample_asqa_5.jsonl
custom: {}
evaluation:
  metrics:
  - acc
  - f1
  - em
  - coverem
  - stringem
  - rouge-1
  - rouge-2
  - rouge-l
  save_path: output/asqa.json
generation:
  base_url: http://localhost:8000/v1
  model_name: openbmb/MiniCPM4-8B
  sampling_params:
    extra_body:
      chat_template_kwargs:
        enable_thinking: false
      include_stop_str_in_output: true
      top_k: 20
    max_tokens: 2048
    temperature: 0.7
    top_p: 0.8
prompt:
  template: prompt/qa_boxed.jinja
retriever:
  query_instruction: 'Query: '
  retriever_url: http://localhost:8080
  top_k: 5
```

## Step 5: Run Your Reasoning Workflow!

Once everything is ready, run the following command to start the reasoning workflow:

```shell
ultrarag run examples/IterRetGen.yaml
```